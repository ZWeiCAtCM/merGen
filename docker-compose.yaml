name: mergen

# Stable diffusion service base
x-base_service: &base_service
    container_name: mergen-sd-webui-api-container
    ports:
      - "${SD_API_PORT}:${SD_API_PORT}"
    hostname: ${SD_API_HOST}
    volumes:
      - &v1 ./sd-webui-api/data:/data
      - &v2 ./sd-webui-api/output:/output
    env_file:
      - .env
    stop_signal: SIGKILL
    tty: true
    deploy:
      resources:
        reservations:
          devices:
              - driver: nvidia
                device_ids: ['0']
                capabilities: [compute, utility]

services:
# Stable diffusion service
  sd-webui-api: &automatic
    <<: *base_service
    profiles: ["gateway_sd"]
    build: ./sd-webui-api/services/AUTOMATIC1111
    image: mergen-sd-webui-api-image
    environment:
      - CLI_ARGS=--allow-code --medvram --xformers --enable-insecure-extension-access --api
    networks:
      - mergen-network
  download:
    build: ./sd-webui-api/services/download/
    profiles: ["download"]
    volumes:
      - *v1
# API gateway
  server:
    profiles: ["gateway_sd"]
    build: 
      context: ./server
      dockerfile: Dockerfile
    image: mergen-server-image
    container_name: mergen-server-container
    env_file:
      - .env
    volumes:
      - ./server:/app
      - ../Frontend/Assets/Material:/app/Material 
    ports:
      - "${GATEWAY_PORT}:${GATEWAY_PORT}"
    networks:
      - mergen-network
# llama 3.2 service
  llama-3.2-1B-api:
    build: 
      context: ./llama_api
      dockerfile: Dockerfile
    image: mergen-llama-service
    container_name: mergen-llama-3.2-1B-container
    env_file:
      - .env
    volumes:
      - ./llama_api:/app
    ports:
      - "${LLAMA_API_PORT}:${LLAMA_API_PORT}"
    hostname: ${LLAMA_API_HOST}

# llama-stack-server
  llama-stack-server:
    image: llamastack/distribution-together  # 使用 Together 版本的 Llama Stack
    container_name: llama-stack-together-container
    env_file:
      - .env
    ports:
      - "${LLAMA_STACK_PORT}:${LLAMA_STACK_PORT}"
    environment:
      - LLAMA_STACK_PORT=${LLAMA_STACK_PORT}
      - TOGETHER_API_KEY=${TOGETHER_API_KEY}  # 从 .env 读取 API Key
    networks:
      - mergen-network
    restart: always  # 让容器在崩溃时自动重启
    # command: ["--port", "5001"]


  ngrok:
      image: ngrok/ngrok:latest
      container_name: ngrok_tunnel
      env_file:
      - .env
      command: ["http", "server:8000"]  # ✅ 把 `server` 服务暴露到公网
      ports:
        - "4040:4040"  # ✅ 访问 `ngrok` Web 界面，查看公网 URL
      environment:
        - NGROK_AUTHTOKEN=${NGROK_API_KEY}  # ✅ 你的 ngrok 认证 token（从 ngrok 官网获取）
      networks:
      - mergen-network

# interior-design-assistant
  interior-design-assistant:
    build:
      context: ./llama-stack-apps
      dockerfile: Dockerfile
    image: mergen-ida-service
    container_name: mergen-ida-container
    env_file:
      - .env
    environment:
      - CUDA_VISIBLE_DEVICES=0  # 如果你使用 GPU
    ports:
      - "${IDA_API_PORT}:${IDA_API_PORT}"
    networks:
      - mergen-network
    volumes:
      - ./llama-stack-apps:/app  # 将本地目录挂载到容器
    depends_on:
      - llama-stack-server
# network
networks:
  mergen-network:
    driver: bridge
